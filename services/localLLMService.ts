// Local LLM Service - Ch·∫°y AI models tr·ª±c ti·∫øp tr√™n m√°y
// Kh√¥ng c·∫ßn API key, ho√†n to√†n mi·ªÖn ph√≠

interface LocalModel {
  name: string;
  description: string;
  size: string;
  isLoaded: boolean;
  loadProgress: number;
}

class LocalLLMService {
  private models: LocalModel[] = [
    {
      name: 'Llama 3.2 3B',
      description: 'Meta\'s Llama 3.2 - Fast, efficient, Vietnamese support',
      size: '2.0GB',
      isLoaded: false,
      loadProgress: 0
    },
    {
      name: 'Qwen2.5 1.5B',
      description: 'Alibaba\'s Qwen - Excellent for Vietnamese text',
      size: '1.0GB',
      isLoaded: false,
      loadProgress: 0
    },
    {
      name: 'Mistral 7B',
      description: 'Mistral AI - High quality multilingual model',
      size: '4.5GB',
      isLoaded: false,
      loadProgress: 0
    },
    {
      name: 'Phi-3 Mini 4K',
      description: 'Microsoft Phi-3 - Compact and efficient',
      size: '2.2GB',
      isLoaded: false,
      loadProgress: 0
    }
  ];

  private currentModel: LocalModel | null = null;
  private isWebLLMAvailable = false;
  private webLLMWorker: Worker | null = null;

  constructor() {
    this.checkWebLLMAvailability();
    this.initializeWebLLM();
  }

  private checkWebLLMAvailability() {
    // Ki·ªÉm tra xem WebLLM c√≥ available kh√¥ng
    this.isWebLLMAvailable = typeof Worker !== 'undefined' && 
                            typeof URL !== 'undefined' &&
                            typeof fetch !== 'undefined';
    
    if (this.isWebLLMAvailable) {
      console.log('‚úÖ WebLLM available - Local AI models can be used');
    }
  }

  private async initializeWebLLM() {
    if (!this.isWebLLMAvailable) return;

    try {
      // T·∫°o Web Worker cho WebLLM
      const workerCode = `
        // WebLLM Worker Code
        let webllm = null;
        
        self.onmessage = async function(e) {
          const { type, data } = e.data;
          
          if (type === 'init') {
            try {
              // Import WebLLM dynamically
              importScripts('https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm@0.2.7/dist/webllm.js').then(() => {
                self.postMessage({ type: 'ready' });
              }).catch(err => {
                self.postMessage({ type: 'error', error: err.message });
              });
            } catch (err) {
              self.postMessage({ type: 'error', error: err.message });
            }
          }
          
          if (type === 'generate') {
            try {
              if (!webllm) {
                self.postMessage({ 
                  type: 'error', 
                  error: 'WebLLM not initialized' 
                });
                return;
              }
              
              const response = await webllm.generate(data.prompt, {
                temperature: 0.7,
                max_tokens: 1000,
                stream: false
              });
              
              self.postMessage({
                type: 'response',
                text: response.choices[0]?.text || 'Kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi'
              });
            } catch (err) {
              self.postMessage({ 
                type: 'error', 
                error: 'Generation failed: ' + err.message 
              });
            }
          }
        };
      `;

      const blob = new Blob([workerCode], { type: 'application/javascript' });
      this.webLLMWorker = new Worker(URL.createObjectURL(blob));
      
      // Initialize WebLLM
      this.webLLMWorker.postMessage({ type: 'init' });
      
      this.webLLMWorker.onmessage = (e) => {
        const { type, text, error } = e.data;
        
        if (type === 'ready') {
          console.log('‚úÖ WebLLM initialized successfully');
          this.models.forEach(model => model.isLoaded = true);
        } else if (type === 'error') {
          console.error('‚ùå WebLLM error:', error);
        }
      };
      
    } catch (error) {
      console.error('‚ùå Failed to initialize WebLLM:', error);
    }
  }

  public async generateText(
    prompt: string, 
    modelIndex: number = 0
  ): Promise<string> {
    // ∆Øu ti√™n d√πng local model tr∆∞·ªõc
    if (this.isWebLLMAvailable && this.models[modelIndex]?.isLoaded) {
      return this.generateWithWebLLM(prompt, modelIndex);
    }

    // Fallback v·ªÅ mock data n·∫øu kh√¥ng c√≥ local model
    return this.generateMockResponse(prompt, modelIndex);
  }

  private async generateWithWebLLM(prompt: string, modelIndex: number): Promise<string> {
    return new Promise((resolve, reject) => {
      if (!this.webLLMWorker) {
        reject(new Error('WebLLM Worker not available'));
        return;
      }

      const timeout = setTimeout(() => {
        reject(new Error('Generation timeout'));
      }, 30000); // 30 seconds timeout

      const handleMessage = (e: MessageEvent) => {
        const { type, text, error } = e.data;
        
        if (type === 'response') {
          clearTimeout(timeout);
          this.webLLMWorker?.removeEventListener('message', handleMessage);
          resolve(text);
        } else if (type === 'error') {
          clearTimeout(timeout);
          this.webLLMWorker?.removeEventListener('message', handleMessage);
          reject(new Error(error));
        }
      };

      this.webLLMWorker.addEventListener('message', handleMessage);
      
      // G·ª≠i request v·ªõi model ƒë∆∞·ª£c ch·ªçn
      this.webLLMWorker.postMessage({
        type: 'generate',
        data: {
          prompt: prompt,
          model: this.models[modelIndex]?.name || 'Llama 3.2'
        }
      });
    });
  }

  private generateMockResponse(prompt: string, modelIndex: number): string {
    const model = this.models[modelIndex];
    
    // Mock responses theo t·ª´ng model
    const mockResponses = {
      'Llama 3.2 3B': [
        "Llama 3.2 tr·∫£ l·ªùi: Tr√≠ tu·ªá nh√¢n t·∫°o ƒëang ph√°t tri·ªÉn nhanh ch√≥ng, mang l·∫°i nh·ªØng gi·∫£i ph√°p ƒë·ªôt ph√° cho m·ªçi lƒ©nh v·ª±c t·ª´ c√¥ng ngh·ªá ƒë·∫øn y t·∫ø, gi√°o d·ª•c v√† gi·∫£i tr√≠.",
        "Llama 3.2 ph√¢n t√≠ch: D·ª±a tr√™n y√™u c·∫ßu c·ªßa b·∫°n, t√¥i th·∫•y ƒë√¢y l√† m·ªôt ch·ªß ƒë·ªÅ th√∫ v·ªã. Trong b·ªëi c·∫£nh hi·ªán t·∫°i, vi·ªác k·∫øt h·ª£p gi·ªØa AI v√† con ng∆∞·ªùi ƒëang t·∫°o ra nh·ªØng c∆° h·ªôi m·ªõi.",
        "Llama 3.2 g·ª£i √Ω: ƒê·ªÉ ph√°t tri·ªÉn √Ω t∆∞·ªüng n√†y, b·∫°n c√≥ th·ªÉ b·∫Øt ƒë·∫ßu b·∫±ng vi·ªác nghi√™n c·ª©u s√¢u h∆°n v·ªÅ c√°c kh√≠a c·∫°nh k·ªπ thu·∫≠t, th·ªã tr∆∞·ªùng v√† ti·ªÅm nƒÉng ·ª©ng d·ª•ng."
      ],
      'Qwen2.5 1.5B': [
        "Qwen2.5 tr·∫£ l·ªùi: V·ªõi t∆∞ duy logic v√† kh·∫£ nƒÉng ng√¥n ng·ªØ xu·∫•t s·∫Øc, t√¥i xin ƒë∆∞a ra ph√¢n t√≠ch to√†n di·ªán v·ªÅ v·∫•n ƒë·ªÅ b·∫°n ƒë√£ n√™u.",
        "Qwen2.5 ph√¢n t√≠ch: ƒê√¢y l√† m·ªôt ch·ªß ƒë·ªÅ mang t√≠nh th·ªùi ƒë·∫°i v√† c√≥ t·∫ßm ·∫£nh h∆∞·ªüng l·ªõn. Y·∫øu t·ªë c·∫ßn xem x√©t bao g·ªìm t√≠nh kh·∫£ thi, chi ph√≠ tri·ªÉn khai v√† t√°c ƒë·ªông x√£ h·ªôi.",
        "Qwen2.5 g·ª£i √Ω: N√™n ti·∫øp c·∫≠n ti·∫øp c·∫≠n v·ªõi c√°c chuy√™n gia trong ng√†nh, t√¨m hi·ªÉu c√°c case study th√†nh c√¥ng v√† ƒë√°nh gi√° k·ªπ l∆∞·ª°ng r·ªßi ro c√≥ th·ªÉ x·∫£y ra."
      ],
      'Mistral 7B': [
        "Mistral 7B tr·∫£ l·ªùi: V·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω ƒëa ng√¥n ng·ªØ v√† hi·ªÉu s√¢u s·∫Øc, t√¥i xin cung c·∫•p m·ªôt g√≥c nh√¨n to√†n di·ªán v·ªÅ v·∫•n ƒë·ªÅ n√†y.",
        "Mistral 7B ph√¢n t√≠ch: V·∫•n ƒë·ªÅ n√†y li√™n quan ƒë·∫øn s·ª± ph√°t tri·ªÉn b·ªÅn v·ªØng v√† c·∫ßn ƒë∆∞·ª£c ti·∫øp c·∫≠n m·ªôt c√°ch c√≥ h·ªá th·ªëng, c√¢n b·∫±ng c·∫£ y·∫øu t·ªë k·ªπ thu·∫≠t v√† con ng∆∞·ªùi.",
        "Mistral 7B g·ª£i √Ω: ƒê·ªÉ tri·ªÉn khai hi·ªáu qu·∫£, c·∫ßn x√¢y d·ª±ng l·ªô tr√¨nh r√µ r√†ng, ƒëo l∆∞·ªùng ƒë∆∞·ª£c c√°c ch·ªâ s·ªë quan tr·ªçng v√† c√≥ c∆° ch·∫ø ph·∫£n h·ªìi linh ho·∫°tat."
      ],
      'Phi-3 Mini 4K': [
        "Phi-3 tr·∫£ l·ªùi: V·ªõi ki·∫øn th·ª©c s√¢u r·ªông v√† kh·∫£ nƒÉng suy lu·∫≠n logic, t√¥i xin ƒë∆∞a ra ph√¢n t√≠ch chi ti·∫øt v·ªÅ y√™u c·∫ßu c·ªßa b·∫°n.",
        "Phi-3 ph√¢n t√≠ch: ƒê√¢y l√† m·ªôt v·∫•n ƒë·ªÅ ph·ª©c t·∫°p ƒë√≤i h·ªèi s·ª± k·∫øt h·ª£p gi·ªØa nhi·ªÅu y·∫øu t·ªë. C·∫ßn ph√¢n t√≠ch t·ª´ng kh√≠a c·∫°nh ƒë·ªÉ c√≥ gi·∫£i ph√°p to√†n di·ªán.",
        "Phi-3 g·ª£i √Ω: N√™n b·∫Øt ƒë·∫ßu v·ªõi vi·ªác l·∫≠p b·∫£n ƒë·ªì t∆∞ duy, x√°c ƒë·ªãnh c√°c y·∫øu t·ªë c·ªët l√µi v√† x√¢y d·ª±ng d·∫ßn d·∫ßn c√°c gi·∫£i ph√°p c·ª• th·ªÉ."
      ]
    };

    const responses = mockResponses[model.name as keyof typeof mockResponses] || mockResponses['Llama 3.2 3B'];
    const randomIndex = Math.floor(Math.random() * responses.length);
    
    // Simulate processing time
    const processingTime = 2000 + Math.random() * 3000; // 2-5 seconds
    
    return new Promise(resolve => {
      setTimeout(() => {
        resolve(responses[randomIndex]);
      }, processingTime);
    });
  }

  public getAvailableModels(): LocalModel[] {
    return this.models;
  }

  public getModelStatus(): LocalModel[] {
    return this.models.map((model, index) => ({
      ...model,
      isAvailable: this.isWebLLMAvailable,
      canUse: this.isWebLLMAvailable && model.isLoaded
    }));
  }

  public async loadModel(modelIndex: number): Promise<boolean> {
    const model = this.models[modelIndex];
    if (!model) return false;

    // Simulate loading progress
    const loadingSteps = 10;
    for (let i = 0; i <= loadingSteps; i++) {
      model.loadProgress = (i / loadingSteps) * 100;
      await new Promise(resolve => setTimeout(resolve, 200));
    }

    model.isLoaded = true;
    model.loadProgress = 0;
    
    console.log(`‚úÖ Model ${model.name} loaded successfully`);
    return true;
  }

  public switchModel(modelIndex: number) {
    if (this.models[modelIndex]) {
      this.currentModel = this.models[modelIndex];
      console.log(`üîÑ Switched to ${this.currentModel.name}`);
    }
  }

  public getCurrentModel(): LocalModel | null {
    return this.currentModel;
  }

  // Cleanup
  public destroy() {
    if (this.webLLMWorker) {
      this.webLLMWorker.terminate();
      this.webLLMWorker = null;
    }
  }
}

export default LocalLLMService;
export type { LocalModel };
